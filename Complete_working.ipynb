{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"/kaggle/input/train-data/train.csv\")\n",
    "test_data = pd.read_csv(\"/kaggle/input/test-data/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming train_data is already loaded\n",
    "train_data.columns = train_data.columns.str.strip()\n",
    "\n",
    "# List of features that are conditional on the existence of something\n",
    "conditional_mapping = {\n",
    "    'PoolQuality': 'SwimmingPoolArea',          # Only fill if pool exists\n",
    "    'BasementHeight': 'BasementTotalSF',\n",
    "    'BasementCondition': 'BasementTotalSF',\n",
    "    'BasementExposure': 'BasementTotalSF',\n",
    "    'BasementFacilityType1': 'BasementTotalSF',\n",
    "    'BasementFacilityType2': 'BasementTotalSF',\n",
    "    'LoungeQuality': 'Lounges',\n",
    "    'ParkingType': 'ParkingArea',\n",
    "    'ParkingFinish': 'ParkingArea',\n",
    "    'ParkingQuality': 'ParkingArea',\n",
    "    'ParkingCondition': 'ParkingArea',\n",
    "    'ExtraFacility': 'ExtraFacilityValue'\n",
    "}\n",
    "\n",
    "# Fill missing values for conditional features\n",
    "for feature, exist_col in conditional_mapping.items():\n",
    "    exists_mask = train_data[exist_col] > 0  # Where the feature logically exists\n",
    "    \n",
    "    # Fill numerical features with median, categorical/text with 'None' or 'Unknown'\n",
    "    if train_data[feature].dtype in ['int64', 'float64']:\n",
    "        median_val = train_data.loc[exists_mask, feature].median()\n",
    "        train_data.loc[exists_mask, feature] = train_data.loc[exists_mask, feature].fillna(median_val)\n",
    "        # Where it does NOT exist, fill 0\n",
    "        train_data.loc[~exists_mask, feature] = train_data.loc[~exists_mask, feature].fillna(0)\n",
    "    else:\n",
    "        # Fill with 'Unknown' where feature exists\n",
    "        train_data.loc[exists_mask, feature] = train_data.loc[exists_mask, feature].fillna('Unknown')\n",
    "        # Fill with 'None' where feature does NOT exist\n",
    "        train_data.loc[~exists_mask, feature] = train_data.loc[~exists_mask, feature].fillna('None')\n",
    "\n",
    "# Fill remaining missing numerical columns with median\n",
    "num_cols = train_data.select_dtypes(include=['int64', 'float64']).columns\n",
    "for col in num_cols:\n",
    "    if train_data[col].isnull().sum() > 0:\n",
    "        train_data[col] = train_data[col].fillna(train_data[col].median())\n",
    "\n",
    "# Fill remaining missing categorical columns with mode\n",
    "cat_cols = train_data.select_dtypes(include=['object']).columns\n",
    "for col in cat_cols:\n",
    "    if train_data[col].isnull().sum() > 0:\n",
    "        train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n",
    "\n",
    "# Verify no more missing values\n",
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best case Data Pre-processing used for all models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Encoding Section (after your preprocessing) ---\n",
    "train_data_encoded = train_data.copy()\n",
    "\n",
    "# --- Ordinal mappings ---\n",
    "ordinal_maps = {\n",
    "    'PoolQuality': {'None':0, 'Fa':1, 'Gd':2, 'Ex':3},\n",
    "    'BasementHeight': {'None':0, 'Fa':1, 'TA':2, 'Gd':3, 'Ex':4},\n",
    "    'BasementCondition': {'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5},\n",
    "    'BasementExposure': {'None':0, 'No':1, 'Mn':2, 'Av':3, 'Gd':4, 'Unknown':2},\n",
    "    'LoungeQuality': {'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5},\n",
    "    'ParkingFinish': {'None':0, 'Unf':1, 'RFn':2, 'Fin':3},\n",
    "    'ParkingQuality': {'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5},\n",
    "    'ParkingCondition': {'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5},\n",
    "    'BasementFacilityType1': {'None':0, 'Unf':1, 'LwQ':2, 'ALQ':3, 'Rec':4, 'GLQ':5, 'BLQ':6},\n",
    "    'BasementFacilityType2': {'None':0, 'Unf':1, 'LwQ':2, 'ALQ':3, 'Rec':4, 'GLQ':5, 'BLQ':6},\n",
    "    'ExteriorQuality': {'Fa':0, 'TA':1, 'Gd':2, 'Ex':3},\n",
    "    'ExteriorCondition': {'Po':0, 'Fa':1, 'TA':2, 'Gd':3, 'Ex':4},\n",
    "    'HeatingQuality': {'Po':0, 'Fa':1, 'TA':2, 'Gd':3, 'Ex':4},\n",
    "    'KitchenQuality': {'Fa':0, 'TA':1, 'Gd':2, 'Ex':3},\n",
    "    'PropertyFunctionality': {'Sev':0, 'Min2':1, 'Min1':2, 'Mod':3, 'Typ':4, 'Maj1':5, 'Maj2':6}\n",
    "}\n",
    "\n",
    "# Apply ordinal encoding\n",
    "for col, mapping in ordinal_maps.items():\n",
    "    if col in train_data_encoded.columns:\n",
    "        train_data_encoded[col] = train_data_encoded[col].map(mapping)\n",
    "\n",
    "# --- One-hot encode remaining nominal columns ---\n",
    "cat_cols = train_data_encoded.select_dtypes(include=['object']).columns\n",
    "nominal_cols = [c for c in cat_cols if c not in ordinal_maps.keys()]\n",
    "train_data_encoded = pd.get_dummies(train_data_encoded, columns=nominal_cols, drop_first=True)\n",
    "\n",
    "print(\"‚úÖ Text encoding completed successfully.\")\n",
    "print(\"Encoded data shape:\", train_data_encoded.shape)\n",
    "print(\"Remaining NaN values:\", train_data_encoded.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Encoding done use ordinal mapping and One-hot encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data_encoded.drop( \"Id\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The column Id dropped as it is unnecessary for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"use_inf_as_na option is deprecated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Distribution of the target\n",
    "sns.histplot(train_data_encoded['HotelValue'], bins=50, kde=True)\n",
    "plt.title(\"Distribution of HotelValue\")\n",
    "plt.xlabel(\"HotelValue ($)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Log-transform if skewed\n",
    "sns.histplot(np.log1p(train_data_encoded['HotelValue']), bins=50, kde=True)\n",
    "plt.title(\"Log-Transformed HotelValue\")\n",
    "plt.xlabel(\"Log(HotelValue + 1)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The initial plot was not a centred-gaussian and had a lot of variance. Log transform was done to get a gaussian distribution with minimal variance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Distribution of the target\n",
    "sns.histplot(train_data_encoded['HotelValue'], bins=50, kde=True)\n",
    "plt.title(\"Distribution of HotelValue\")\n",
    "plt.xlabel(\"HotelValue ($)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features = train_data_encoded.drop('HotelValue', axis=1)\n",
    "target = train_data_encoded['HotelValue']\n",
    "\n",
    "# Fit and transform using StandardScaler\n",
    "features_scaled = pd.DataFrame(scaler.fit_transform(features), columns=features.columns)\n",
    "\n",
    "# Combine with target\n",
    "train_data_scaled = pd.concat([features_scaled, target.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying standard scaler on data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Features and target\n",
    "X = train_data_encoded.drop('HotelValue', axis=1)\n",
    "y = train_data_encoded['HotelValue']\n",
    "\n",
    "# Initialize XGBoost regressor\n",
    "xgb_model = XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=5, random_state=42)\n",
    "\n",
    "# Train on full dataset\n",
    "xgb_model.fit(X, y)\n",
    "\n",
    "# Predict on the same training data\n",
    "y_pred_train = xgb_model.predict(X)\n",
    "\n",
    "# Evaluate fit on training data\n",
    "rmse_train = np.sqrt(mean_squared_error(y, y_pred_train))\n",
    "r2_train = r2_score(y, y_pred_train)\n",
    "\n",
    "print(f\"Training RMSE: {rmse_train}\")\n",
    "print(f\"Training R2 Score: {r2_train}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying xgboosting model to the train data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Features and target\n",
    "X = train_data_encoded.drop('HotelValue', axis=1)\n",
    "y = train_data_encoded['HotelValue']\n",
    "\n",
    "# Initialize AdaBoost regressor\n",
    "ada_model = AdaBoostRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train on full dataset\n",
    "ada_model.fit(X, y)\n",
    "\n",
    "# Predict on the same training data\n",
    "y_pred_train = ada_model.predict(X)\n",
    "\n",
    "# Evaluate fit on training data\n",
    "rmse_train = np.sqrt(mean_squared_error(y, y_pred_train))\n",
    "r2_train = r2_score(y, y_pred_train)\n",
    "\n",
    "print(f\"Training RMSE: {rmse_train}\")\n",
    "print(f\"Training R2 Score: {r2_train}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying Adaboosting model to the train data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Features and target\n",
    "X = train_data_encoded.drop('HotelValue', axis=1)\n",
    "y = train_data_encoded['HotelValue']\n",
    "\n",
    "# Initialize Gradient Boosting Regressor\n",
    "gbr_model = GradientBoostingRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train on full dataset\n",
    "gbr_model.fit(X, y)\n",
    "\n",
    "# Predict on the same training data\n",
    "y_pred_train = gbr_model.predict(X)\n",
    "\n",
    "# Evaluate fit on training data\n",
    "rmse_train = np.sqrt(mean_squared_error(y, y_pred_train))\n",
    "r2_train = r2_score(y, y_pred_train)\n",
    "\n",
    "print(f\"Training RMSE: {rmse_train}\")\n",
    "print(f\"Training R2 Score: {r2_train}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying Gradient Boosting model to the train data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Features and target\n",
    "X = train_data_encoded.drop('HotelValue', axis=1)\n",
    "y = train_data_encoded['HotelValue']\n",
    "\n",
    "# Initialize Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # use all CPU cores for faster training\n",
    ")\n",
    "\n",
    "# Train on full dataset\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# Predict on the same training data\n",
    "y_pred_train = rf_model.predict(X)\n",
    "\n",
    "# Evaluate fit on training data\n",
    "rmse_train = np.sqrt(mean_squared_error(y, y_pred_train))\n",
    "r2_train = r2_score(y, y_pred_train)\n",
    "\n",
    "print(f\"Training RMSE: {rmse_train}\")\n",
    "print(f\"Training R2 Score: {r2_train}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying Random Forrest model on the train data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming test_data is already loaded\n",
    "test_data.columns = test_data.columns.str.strip()\n",
    "\n",
    "# List of features that are conditional on the existence of something\n",
    "conditional_mapping = {\n",
    "    'PoolQuality': 'SwimmingPoolArea',          # Only fill if pool exists\n",
    "    'BasementHeight': 'BasementTotalSF',\n",
    "    'BasementCondition': 'BasementTotalSF',\n",
    "    'BasementExposure': 'BasementTotalSF',\n",
    "    'BasementFacilityType1': 'BasementTotalSF',\n",
    "    'BasementFacilityType2': 'BasementTotalSF',\n",
    "    'LoungeQuality': 'Lounges',\n",
    "    'ParkingType': 'ParkingArea',\n",
    "    'ParkingFinish': 'ParkingArea',\n",
    "    'ParkingQuality': 'ParkingArea',\n",
    "    'ParkingCondition': 'ParkingArea',\n",
    "    'ExtraFacility': 'ExtraFacilityValue'\n",
    "}\n",
    "\n",
    "# Fill missing values for conditional features\n",
    "for feature, exist_col in conditional_mapping.items():\n",
    "    exists_mask = test_data[exist_col] > 0  # Where the feature logically exists\n",
    "    \n",
    "    # Fill numerical features with median, categorical/text with 'None' or 'Unknown'\n",
    "    if test_data[feature].dtype in ['int64', 'float64']:\n",
    "        median_val = test_data.loc[exists_mask, feature].median()\n",
    "        test_data.loc[exists_mask, feature] = test_data.loc[exists_mask, feature].fillna(median_val)\n",
    "        # Where it does NOT exist, fill 0\n",
    "        test_data.loc[~exists_mask, feature] = test_data.loc[~exists_mask, feature].fillna(0)\n",
    "    else:\n",
    "        # Fill with 'Unknown' where feature exists\n",
    "        test_data.loc[exists_mask, feature] = test_data.loc[exists_mask, feature].fillna('Unknown')\n",
    "        # Fill with 'None' where feature does NOT exist\n",
    "        test_data.loc[~exists_mask, feature] = test_data.loc[~exists_mask, feature].fillna('None')\n",
    "\n",
    "# Fill remaining missing numerical columns with median\n",
    "num_cols = test_data.select_dtypes(include=['int64', 'float64']).columns\n",
    "for col in num_cols:\n",
    "    if test_data[col].isnull().sum() > 0:\n",
    "        test_data[col].fillna(test_data[col].median(), inplace=True)\n",
    "\n",
    "# Fill remaining missing categorical columns with mode\n",
    "cat_cols = test_data.select_dtypes(include=['object']).columns\n",
    "for col in cat_cols:\n",
    "    if test_data[col].isnull().sum() > 0:\n",
    "        test_data[col].fillna(test_data[col].mode()[0], inplace=True)\n",
    "\n",
    "# Verify no more missing values\n",
    "print(test_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing for test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Encoding Section for test data ---\n",
    "test_data_encoded = test_data.copy()\n",
    "\n",
    "# --- Ordinal mappings (same as training) ---\n",
    "ordinal_maps = {\n",
    "    'PoolQuality': {'None':0, 'Fa':1, 'Gd':2, 'Ex':3},\n",
    "    'BasementHeight': {'None':0, 'Fa':1, 'TA':2, 'Gd':3, 'Ex':4},\n",
    "    'BasementCondition': {'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5},\n",
    "    'BasementExposure': {'None':0, 'No':1, 'Mn':2, 'Av':3, 'Gd':4, 'Unknown':2},\n",
    "    'LoungeQuality': {'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5},\n",
    "    'ParkingFinish': {'None':0, 'Unf':1, 'RFn':2, 'Fin':3},\n",
    "    'ParkingQuality': {'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5},\n",
    "    'ParkingCondition': {'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5},\n",
    "    'BasementFacilityType1': {'None':0, 'Unf':1, 'LwQ':2, 'ALQ':3, 'Rec':4, 'GLQ':5, 'BLQ':6},\n",
    "    'BasementFacilityType2': {'None':0, 'Unf':1, 'LwQ':2, 'ALQ':3, 'Rec':4, 'GLQ':5, 'BLQ':6},\n",
    "    'ExteriorQuality': {'Fa':0, 'TA':1, 'Gd':2, 'Ex':3},\n",
    "    'ExteriorCondition': {'Po':0, 'Fa':1, 'TA':2, 'Gd':3, 'Ex':4},\n",
    "    'HeatingQuality': {'Po':0, 'Fa':1, 'TA':2, 'Gd':3, 'Ex':4},\n",
    "    'KitchenQuality': {'Fa':0, 'TA':1, 'Gd':2, 'Ex':3},\n",
    "    'PropertyFunctionality': {'Sev':0, 'Min2':1, 'Min1':2, 'Mod':3, 'Typ':4, 'Maj1':5, 'Maj2':6}\n",
    "}\n",
    "\n",
    "# Apply ordinal encoding\n",
    "for col, mapping in ordinal_maps.items():\n",
    "    if col in test_data_encoded.columns:\n",
    "        test_data_encoded[col] = test_data_encoded[col].map(mapping)\n",
    "\n",
    "# --- One-hot encode remaining nominal columns ---\n",
    "cat_cols = test_data_encoded.select_dtypes(include=['object']).columns\n",
    "nominal_cols = [c for c in cat_cols if c not in ordinal_maps.keys()]\n",
    "test_data_encoded = pd.get_dummies(test_data_encoded, columns=nominal_cols, drop_first=True)\n",
    "\n",
    "# Ensure the test set has the same columns as the train set\n",
    "for col in train_data_encoded.columns:\n",
    "    if col not in test_data_encoded.columns:\n",
    "        test_data_encoded[col] = 0  # Add missing columns with zeros\n",
    "\n",
    "# Reorder columns to match train data\n",
    "test_data_encoded = test_data_encoded[train_data_encoded.columns.drop('HotelValue')]\n",
    "\n",
    "print(\"‚úÖ Test data encoding completed successfully.\")\n",
    "print(\"Encoded test data shape:\", test_data_encoded.shape)\n",
    "print(\"Remaining NaN values:\", test_data_encoded.isnull().sum().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data encoding for test data using the same method as before**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "row_idx, col_idx = np.where(test_data_encoded.isna())\n",
    "for r, c in zip(row_idx, col_idx):\n",
    "    print(f\"NaN at row {r}, column '{test_data_encoded.columns[c]}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Fill NaN in 'BasementFacilityType2' with previous row's value\n",
    "test_data_encoded['BasementFacilityType2'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Verify no NaNs remain\n",
    "print(\"Remaining NaNs:\", test_data_encoded.isna().sum().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing all NaN values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_data_copy = test_data_encoded.copy()\n",
    "\n",
    "# --- 2. Drop the 'Id' column from the copy (creates a new DataFrame without Id) ---\n",
    "test_data_encoded_no_id = test_data_copy.drop(\"Id\", axis=1)\n",
    "\n",
    "# --- 3. Optional: preview the first few rows ---\n",
    "print(\"Shape without Id:\", test_data_encoded_no_id.shape)\n",
    "print(test_data_encoded_no_id.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Id column as it is unnecessary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Predict using trained XGBoost model ---\n",
    "scaler.transform(test_data_encoded)  # drop 'Id' if present in features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_pred = xgb_model.predict(test_data_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting values using xgboosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Assuming `test_data` (or `test_processed`) still has the 'Id' column\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_data_copy['Id'],       # bring back Hotel ID\n",
    "    'HotelValue': y_pred         # predicted values\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission_df.to_csv('hotelvalue_submission.csv', index=False)\n",
    "\n",
    "# Preview first few rows\n",
    "print(submission_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_data_encoded.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_pred = gbr_model.predict(test_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Assuming `test_data` (or `test_processed`) still has the 'Id' column\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_data_copy['Id'],       # bring back Hotel ID\n",
    "    'HotelValue': y_pred         # predicted values\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission_df.to_csv('hotelvalue_submission.csv', index=False)\n",
    "\n",
    "# Preview first few rows\n",
    "print(submission_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving the predicted values in a file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(test_data_encoded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Features and target\n",
    "# -----------------------------\n",
    "X = train_data_encoded.drop('HotelValue', axis=1)\n",
    "y = train_data_encoded['HotelValue']\n",
    "\n",
    "# -----------------------------\n",
    "# Log-transform the target to ensure positivity\n",
    "# -----------------------------\n",
    "y_log = np.log1p(y)  # log(1 + y) avoids issues if y=0\n",
    "\n",
    "# -----------------------------\n",
    "# Standardize features\n",
    "# -----------------------------\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# -----------------------------\n",
    "# Apply PCA\n",
    "# -----------------------------\n",
    "pca = PCA(n_components=0.95)  # keep 95% of variance\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Original features: {X.shape[1]}, PCA components: {X_pca.shape[1]}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Train Linear Regression on PCA components\n",
    "# -----------------------------\n",
    "lin_model = LinearRegression()\n",
    "lin_model.fit(X_pca, y_log)\n",
    "\n",
    "# -----------------------------\n",
    "# Predict on training data\n",
    "# -----------------------------\n",
    "y_pred_train_log = lin_model.predict(X_pca)\n",
    "y_pred_train = np.expm1(y_pred_train_log)  # invert log-transform\n",
    "\n",
    "# Clip negative values (just in case)\n",
    "y_pred_train = np.maximum(0, y_pred_train)\n",
    "\n",
    "# Evaluate\n",
    "rmse_train = np.sqrt(mean_squared_error(y, y_pred_train))\n",
    "r2_train = r2_score(y, y_pred_train)\n",
    "\n",
    "print(f\"Training RMSE: {rmse_train}\")\n",
    "print(f\"Training R¬≤: {r2_train}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Predict on test data\n",
    "# -----------------------------\n",
    "# Keep Id separately\n",
    "test_ids = test_data_encoded['Id']\n",
    "test_features = test_data_encoded.drop('Id', axis=1)\n",
    "\n",
    "# Ensure test features have same columns as training\n",
    "test_features = test_features.reindex(columns=X.columns, fill_value=0)\n",
    "\n",
    "# Scale and apply PCA\n",
    "X_test_scaled = scaler.transform(test_features)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Predict and invert log-transform\n",
    "y_pred_test_log = lin_model.predict(X_test_pca)\n",
    "y_pred_test = np.expm1(y_pred_test_log)\n",
    "y_pred_test = np.maximum(0, y_pred_test)  # clip negatives\n",
    "\n",
    "# Create final predictions DataFrame with Id and HotelValue\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Id': test_ids,\n",
    "    'HotelValue': y_pred_test\n",
    "})\n",
    "\n",
    "print(predictions_df.head())\n",
    "\n",
    "# Optional: save predictions\n",
    "predictions_df.to_csv('hotel_value_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-linear response and optimizing hyperparameters using Bayesian approach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# View the first few predictions\n",
    "print(predictions_df.head())\n",
    "\n",
    "# Save to CSV for later use\n",
    "predictions_df.to_csv('hotel_value_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Print number of rows and columns\n",
    "print(predictions_df.shape)\n",
    "\n",
    "# Optional: just number of rows\n",
    "print(\"Number of rows:\", predictions_df.shape[0])\n",
    "\n",
    "# Optional: just number of columns\n",
    "print(\"Number of columns:\", predictions_df.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_pred = ada_model.predict(test_data_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting values for Adaboost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Assuming `test_data` (or `test_processed`) still has the 'Id' column\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_data_copy['Id'],       # bring back Hotel ID\n",
    "    'HotelValue': y_pred        # predicted values\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission_df.to_csv('hotelvalue_submission.csv', index=False)\n",
    "\n",
    "# Preview first few rows\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Ridge Regression Model for Hotel Value Prediction\n",
    "# ============================================================\n",
    "\n",
    "ridge_base = Ridge(random_state=42)\n",
    "\n",
    "# --- 5. Parameter grid for tuning ---\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 1.0, 5.0, 10.0, 20.0, 50.0, 100.0],\n",
    "    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'saga'],\n",
    "    'fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "# --- 6. Grid Search with Cross Validation ---\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=ridge_base,\n",
    "    param_grid=param_grid,\n",
    "    scoring='r2',\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nüîç Starting Grid Search for Ridge Regression...\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "grid_search.fit(X_scaled, y_log)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Grid search completed in {elapsed_time/60:.2f} minutes\")\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(f\"Best CV R¬≤: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# --- 7. Final Model with Best Parameters ---\n",
    "best_params = grid_search.best_params_\n",
    "ridge_final = Ridge(random_state=42, **best_params)\n",
    "\n",
    "print(\"\\nüèãÔ∏è‚Äç‚ôÇÔ∏è Training final Ridge Regression model on full dataset...\")\n",
    "ridge_final.fit(X_scaled, y_log)\n",
    "\n",
    "# --- 8. Evaluate on training data ---\n",
    "y_pred_log = ridge_final.predict(X_scaled)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "rmse_train = np.sqrt(np.mean((y - y_pred) ** 2))\n",
    "r2_train = 1 - np.sum((y - y_pred) ** 2) / np.sum((y - np.mean(y)) ** 2)\n",
    "\n",
    "print(\"\\nüìä Final Ridge Model Performance on Full Data:\")\n",
    "print(f\"Training RMSE: {rmse_train:,.4f}\")\n",
    "print(f\"Training R¬≤: {r2_train:.4f}\")\n",
    "\n",
    "# --- 9. Save model and scaler ---\n",
    "joblib.dump(ridge_final, \"fitted_linear_model.joblib\")\n",
    "joblib.dump(scaler, \"ridge_scaler.joblib\")\n",
    "\n",
    "print(\"\\nüíæ Ridge model saved as 'fitted_linear_model.joblib'\")\n",
    "print(\"üíæ Scaler saved as 'ridge_scaler.joblib'\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8576271,
     "sourceId": 13507707,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8576278,
     "sourceId": 13507714,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
