{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13507707,"sourceType":"datasetVersion","datasetId":8576271},{"sourceId":13507714,"sourceType":"datasetVersion","datasetId":8576278}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/train-data/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/test-data/test.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Assuming train_data is already loaded\ntrain_data.columns = train_data.columns.str.strip()\n\n# List of features that are conditional on the existence of something\nconditional_mapping = {\n    'PoolQuality': 'SwimmingPoolArea',          # Only fill if pool exists\n    'BasementHeight': 'BasementTotalSF',\n    'BasementCondition': 'BasementTotalSF',\n    'BasementExposure': 'BasementTotalSF',\n    'BasementFacilityType1': 'BasementTotalSF',\n    'BasementFacilityType2': 'BasementTotalSF',\n    'LoungeQuality': 'Lounges',\n    'ParkingType': 'ParkingArea',\n    'ParkingFinish': 'ParkingArea',\n    'ParkingQuality': 'ParkingArea',\n    'ParkingCondition': 'ParkingArea',\n    'ExtraFacility': 'ExtraFacilityValue'\n}\n\n# Fill missing values for conditional features\nfor feature, exist_col in conditional_mapping.items():\n    exists_mask = train_data[exist_col] > 0  # Where the feature logically exists\n    \n    # Fill numerical features with median, categorical/text with 'None' or 'Unknown'\n    if train_data[feature].dtype in ['int64', 'float64']:\n        median_val = train_data.loc[exists_mask, feature].median()\n        train_data.loc[exists_mask, feature] = train_data.loc[exists_mask, feature].fillna(median_val)\n        # Where it does NOT exist, fill 0\n        train_data.loc[~exists_mask, feature] = train_data.loc[~exists_mask, feature].fillna(0)\n    else:\n        # Fill with 'Unknown' where feature exists\n        train_data.loc[exists_mask, feature] = train_data.loc[exists_mask, feature].fillna('Unknown')\n        # Fill with 'None' where feature does NOT exist\n        train_data.loc[~exists_mask, feature] = train_data.loc[~exists_mask, feature].fillna('None')\n\n# Fill remaining missing numerical columns with median\nnum_cols = train_data.select_dtypes(include=['int64', 'float64']).columns\nfor col in num_cols:\n    if train_data[col].isnull().sum() > 0:\n        train_data[col] = train_data[col].fillna(train_data[col].median())\n\n# Fill remaining missing categorical columns with mode\ncat_cols = train_data.select_dtypes(include=['object']).columns\nfor col in cat_cols:\n    if train_data[col].isnull().sum() > 0:\n        train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n\n# Verify no more missing values\nprint(train_data.isnull().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Best case Data Pre-processing used for all models**","metadata":{}},{"cell_type":"code","source":"# --- Encoding Section (after your preprocessing) ---\ntrain_data_encoded = train_data.copy()\n\n# --- Ordinal mappings ---\nordinal_maps = {\n    'PoolQuality': {'None':0, 'Fa':1, 'Gd':2, 'Ex':3},\n    'BasementHeight': {'None':0, 'Fa':1, 'TA':2, 'Gd':3, 'Ex':4},\n    'BasementCondition': {'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5},\n    'BasementExposure': {'None':0, 'No':1, 'Mn':2, 'Av':3, 'Gd':4, 'Unknown':2},\n    'LoungeQuality': {'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5},\n    'ParkingFinish': {'None':0, 'Unf':1, 'RFn':2, 'Fin':3},\n    'ParkingQuality': {'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5},\n    'ParkingCondition': {'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5},\n    'BasementFacilityType1': {'None':0, 'Unf':1, 'LwQ':2, 'ALQ':3, 'Rec':4, 'GLQ':5, 'BLQ':6},\n    'BasementFacilityType2': {'None':0, 'Unf':1, 'LwQ':2, 'ALQ':3, 'Rec':4, 'GLQ':5, 'BLQ':6},\n    'ExteriorQuality': {'Fa':0, 'TA':1, 'Gd':2, 'Ex':3},\n    'ExteriorCondition': {'Po':0, 'Fa':1, 'TA':2, 'Gd':3, 'Ex':4},\n    'HeatingQuality': {'Po':0, 'Fa':1, 'TA':2, 'Gd':3, 'Ex':4},\n    'KitchenQuality': {'Fa':0, 'TA':1, 'Gd':2, 'Ex':3},\n    'PropertyFunctionality': {'Sev':0, 'Min2':1, 'Min1':2, 'Mod':3, 'Typ':4, 'Maj1':5, 'Maj2':6}\n}\n\n# Apply ordinal encoding\nfor col, mapping in ordinal_maps.items():\n    if col in train_data_encoded.columns:\n        train_data_encoded[col] = train_data_encoded[col].map(mapping)\n\n# --- One-hot encode remaining nominal columns ---\ncat_cols = train_data_encoded.select_dtypes(include=['object']).columns\nnominal_cols = [c for c in cat_cols if c not in ordinal_maps.keys()]\ntrain_data_encoded = pd.get_dummies(train_data_encoded, columns=nominal_cols, drop_first=True)\n\nprint(\"✅ Text encoding completed successfully.\")\nprint(\"Encoded data shape:\", train_data_encoded.shape)\nprint(\"Remaining NaN values:\", train_data_encoded.isnull().sum().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Data Encoding done use ordinal mapping and One-hot encoding**","metadata":{}},{"cell_type":"code","source":"train_data_encoded.drop( \"Id\", axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**The column Id dropped as it is unnecessary for training**","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", message=\"use_inf_as_na option is deprecated\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Distribution of the target\nsns.histplot(train_data_encoded['HotelValue'], bins=50, kde=True)\nplt.title(\"Distribution of HotelValue\")\nplt.xlabel(\"HotelValue ($)\")\nplt.ylabel(\"Count\")\nplt.show()\n\n# Log-transform if skewed\nsns.histplot(np.log1p(train_data_encoded['HotelValue']), bins=50, kde=True)\nplt.title(\"Log-Transformed HotelValue\")\nplt.xlabel(\"Log(HotelValue + 1)\")\nplt.ylabel(\"Count\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**The initial plot was not a centred-gaussian and had a lot of variance. Log transform was done to get a gaussian distribution with minimal variance**","metadata":{}},{"cell_type":"code","source":"# Distribution of the target\nsns.histplot(train_data_encoded['HotelValue'], bins=50, kde=True)\nplt.title(\"Distribution of HotelValue\")\nplt.xlabel(\"HotelValue ($)\")\nplt.ylabel(\"Count\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nfeatures = train_data_encoded.drop('HotelValue', axis=1)\ntarget = train_data_encoded['HotelValue']\n\n# Fit and transform using StandardScaler\nfeatures_scaled = pd.DataFrame(scaler.fit_transform(features), columns=features.columns)\n\n# Combine with target\ntrain_data_scaled = pd.concat([features_scaled, target.reset_index(drop=True)], axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Applying standard scaler on data**","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\n\n# Features and target\nX = train_data_encoded.drop('HotelValue', axis=1)\ny = train_data_encoded['HotelValue']\n\n# Initialize XGBoost regressor\nxgb_model = XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=5, random_state=42)\n\n# Train on full dataset\nxgb_model.fit(X, y)\n\n# Predict on the same training data\ny_pred_train = xgb_model.predict(X)\n\n# Evaluate fit on training data\nrmse_train = np.sqrt(mean_squared_error(y, y_pred_train))\nr2_train = r2_score(y, y_pred_train)\n\nprint(f\"Training RMSE: {rmse_train}\")\nprint(f\"Training R2 Score: {r2_train}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Applying xgboosting model to the train data**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\n\n# Features and target\nX = train_data_encoded.drop('HotelValue', axis=1)\ny = train_data_encoded['HotelValue']\n\n# Initialize AdaBoost regressor\nada_model = AdaBoostRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    random_state=42\n)\n\n# Train on full dataset\nada_model.fit(X, y)\n\n# Predict on the same training data\ny_pred_train = ada_model.predict(X)\n\n# Evaluate fit on training data\nrmse_train = np.sqrt(mean_squared_error(y, y_pred_train))\nr2_train = r2_score(y, y_pred_train)\n\nprint(f\"Training RMSE: {rmse_train}\")\nprint(f\"Training R2 Score: {r2_train}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Applying Adaboosting model to the train data**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\n\n# Features and target\nX = train_data_encoded.drop('HotelValue', axis=1)\ny = train_data_encoded['HotelValue']\n\n# Initialize Gradient Boosting Regressor\ngbr_model = GradientBoostingRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=5,\n    random_state=42\n)\n\n# Train on full dataset\ngbr_model.fit(X, y)\n\n# Predict on the same training data\ny_pred_train = gbr_model.predict(X)\n\n# Evaluate fit on training data\nrmse_train = np.sqrt(mean_squared_error(y, y_pred_train))\nr2_train = r2_score(y, y_pred_train)\n\nprint(f\"Training RMSE: {rmse_train}\")\nprint(f\"Training R2 Score: {r2_train}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Applying Gradient Boosting model to the train data**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\n\n# Features and target\nX = train_data_encoded.drop('HotelValue', axis=1)\ny = train_data_encoded['HotelValue']\n\n# Initialize Random Forest Regressor\nrf_model = RandomForestRegressor(\n    n_estimators=500,\n    max_depth=5,\n    random_state=42,\n    n_jobs=-1  # use all CPU cores for faster training\n)\n\n# Train on full dataset\nrf_model.fit(X, y)\n\n# Predict on the same training data\ny_pred_train = rf_model.predict(X)\n\n# Evaluate fit on training data\nrmse_train = np.sqrt(mean_squared_error(y, y_pred_train))\nr2_train = r2_score(y, y_pred_train)\n\nprint(f\"Training RMSE: {rmse_train}\")\nprint(f\"Training R2 Score: {r2_train}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Applying Random Forrest model on the train data**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Assuming test_data is already loaded\ntest_data.columns = test_data.columns.str.strip()\n\n# List of features that are conditional on the existence of something\nconditional_mapping = {\n    'PoolQuality': 'SwimmingPoolArea',          # Only fill if pool exists\n    'BasementHeight': 'BasementTotalSF',\n    'BasementCondition': 'BasementTotalSF',\n    'BasementExposure': 'BasementTotalSF',\n    'BasementFacilityType1': 'BasementTotalSF',\n    'BasementFacilityType2': 'BasementTotalSF',\n    'LoungeQuality': 'Lounges',\n    'ParkingType': 'ParkingArea',\n    'ParkingFinish': 'ParkingArea',\n    'ParkingQuality': 'ParkingArea',\n    'ParkingCondition': 'ParkingArea',\n    'ExtraFacility': 'ExtraFacilityValue'\n}\n\n# Fill missing values for conditional features\nfor feature, exist_col in conditional_mapping.items():\n    exists_mask = test_data[exist_col] > 0  # Where the feature logically exists\n    \n    # Fill numerical features with median, categorical/text with 'None' or 'Unknown'\n    if test_data[feature].dtype in ['int64', 'float64']:\n        median_val = test_data.loc[exists_mask, feature].median()\n        test_data.loc[exists_mask, feature] = test_data.loc[exists_mask, feature].fillna(median_val)\n        # Where it does NOT exist, fill 0\n        test_data.loc[~exists_mask, feature] = test_data.loc[~exists_mask, feature].fillna(0)\n    else:\n        # Fill with 'Unknown' where feature exists\n        test_data.loc[exists_mask, feature] = test_data.loc[exists_mask, feature].fillna('Unknown')\n        # Fill with 'None' where feature does NOT exist\n        test_data.loc[~exists_mask, feature] = test_data.loc[~exists_mask, feature].fillna('None')\n\n# Fill remaining missing numerical columns with median\nnum_cols = test_data.select_dtypes(include=['int64', 'float64']).columns\nfor col in num_cols:\n    if test_data[col].isnull().sum() > 0:\n        test_data[col].fillna(test_data[col].median(), inplace=True)\n\n# Fill remaining missing categorical columns with mode\ncat_cols = test_data.select_dtypes(include=['object']).columns\nfor col in cat_cols:\n    if test_data[col].isnull().sum() > 0:\n        test_data[col].fillna(test_data[col].mode()[0], inplace=True)\n\n# Verify no more missing values\nprint(test_data.isnull().sum())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Data Preprocessing for test data**","metadata":{}},{"cell_type":"code","source":"# --- Encoding Section for test data ---\ntest_data_encoded = test_data.copy()\n\n# --- Ordinal mappings (same as training) ---\nordinal_maps = {\n    'PoolQuality': {'None':0, 'Fa':1, 'Gd':2, 'Ex':3},\n    'BasementHeight': {'None':0, 'Fa':1, 'TA':2, 'Gd':3, 'Ex':4},\n    'BasementCondition': {'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5},\n    'BasementExposure': {'None':0, 'No':1, 'Mn':2, 'Av':3, 'Gd':4, 'Unknown':2},\n    'LoungeQuality': {'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5},\n    'ParkingFinish': {'None':0, 'Unf':1, 'RFn':2, 'Fin':3},\n    'ParkingQuality': {'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5},\n    'ParkingCondition': {'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5},\n    'BasementFacilityType1': {'None':0, 'Unf':1, 'LwQ':2, 'ALQ':3, 'Rec':4, 'GLQ':5, 'BLQ':6},\n    'BasementFacilityType2': {'None':0, 'Unf':1, 'LwQ':2, 'ALQ':3, 'Rec':4, 'GLQ':5, 'BLQ':6},\n    'ExteriorQuality': {'Fa':0, 'TA':1, 'Gd':2, 'Ex':3},\n    'ExteriorCondition': {'Po':0, 'Fa':1, 'TA':2, 'Gd':3, 'Ex':4},\n    'HeatingQuality': {'Po':0, 'Fa':1, 'TA':2, 'Gd':3, 'Ex':4},\n    'KitchenQuality': {'Fa':0, 'TA':1, 'Gd':2, 'Ex':3},\n    'PropertyFunctionality': {'Sev':0, 'Min2':1, 'Min1':2, 'Mod':3, 'Typ':4, 'Maj1':5, 'Maj2':6}\n}\n\n# Apply ordinal encoding\nfor col, mapping in ordinal_maps.items():\n    if col in test_data_encoded.columns:\n        test_data_encoded[col] = test_data_encoded[col].map(mapping)\n\n# --- One-hot encode remaining nominal columns ---\ncat_cols = test_data_encoded.select_dtypes(include=['object']).columns\nnominal_cols = [c for c in cat_cols if c not in ordinal_maps.keys()]\ntest_data_encoded = pd.get_dummies(test_data_encoded, columns=nominal_cols, drop_first=True)\n\n# Ensure the test set has the same columns as the train set\nfor col in train_data_encoded.columns:\n    if col not in test_data_encoded.columns:\n        test_data_encoded[col] = 0  # Add missing columns with zeros\n\n# Reorder columns to match train data\ntest_data_encoded = test_data_encoded[train_data_encoded.columns.drop('HotelValue')]\n\nprint(\"✅ Test data encoding completed successfully.\")\nprint(\"Encoded test data shape:\", test_data_encoded.shape)\nprint(\"Remaining NaN values:\", test_data_encoded.isnull().sum().sum())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Data encoding for test data using the same method as before**","metadata":{}},{"cell_type":"code","source":"row_idx, col_idx = np.where(test_data_encoded.isna())\nfor r, c in zip(row_idx, col_idx):\n    print(f\"NaN at row {r}, column '{test_data_encoded.columns[c]}'\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fill NaN in 'BasementFacilityType2' with previous row's value\ntest_data_encoded['BasementFacilityType2'].fillna(method='ffill', inplace=True)\n\n# Verify no NaNs remain\nprint(\"Remaining NaNs:\", test_data_encoded.isna().sum().sum())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Removing all NaN values**","metadata":{}},{"cell_type":"code","source":"test_data_copy = test_data_encoded.copy()\n\n# --- 2. Drop the 'Id' column from the copy (creates a new DataFrame without Id) ---\ntest_data_encoded_no_id = test_data_copy.drop(\"Id\", axis=1)\n\n# --- 3. Optional: preview the first few rows ---\nprint(\"Shape without Id:\", test_data_encoded_no_id.shape)\nprint(test_data_encoded_no_id.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Removing Id column as it is unnecessary**","metadata":{}},{"cell_type":"code","source":"# --- Predict using trained XGBoost model ---\nscaler.transform(test_data_encoded)  # drop 'Id' if present in features\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = xgb_model.predict(test_data_encoded)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Predicting values using xgboosting**","metadata":{}},{"cell_type":"code","source":"# Assuming `test_data` (or `test_processed`) still has the 'Id' column\nsubmission_df = pd.DataFrame({\n    'Id': test_data_copy['Id'],       # bring back Hotel ID\n    'HotelValue': y_pred         # predicted values\n})\n\n# Save to CSV\nsubmission_df.to_csv('hotelvalue_submission.csv', index=False)\n\n# Preview first few rows\nprint(submission_df.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data_encoded.columns.tolist()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = gbr_model.predict(test_data_encoded)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming `test_data` (or `test_processed`) still has the 'Id' column\nsubmission_df = pd.DataFrame({\n    'Id': test_data_copy['Id'],       # bring back Hotel ID\n    'HotelValue': y_pred         # predicted values\n})\n\n# Save to CSV\nsubmission_df.to_csv('hotelvalue_submission.csv', index=False)\n\n# Preview first few rows\nprint(submission_df.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Saving the predicted values in a file**","metadata":{}},{"cell_type":"code","source":"print(test_data_encoded.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\nimport pandas as pd\n\n# -----------------------------\n# Features and target\n# -----------------------------\nX = train_data_encoded.drop('HotelValue', axis=1)\ny = train_data_encoded['HotelValue']\n\n# -----------------------------\n# Log-transform the target to ensure positivity\n# -----------------------------\ny_log = np.log1p(y)  # log(1 + y) avoids issues if y=0\n\n# -----------------------------\n# Standardize features\n# -----------------------------\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# -----------------------------\n# Apply PCA\n# -----------------------------\npca = PCA(n_components=0.95)  # keep 95% of variance\nX_pca = pca.fit_transform(X_scaled)\n\nprint(f\"Original features: {X.shape[1]}, PCA components: {X_pca.shape[1]}\")\n\n# -----------------------------\n# Train Linear Regression on PCA components\n# -----------------------------\nlin_model = LinearRegression()\nlin_model.fit(X_pca, y_log)\n\n# -----------------------------\n# Predict on training data\n# -----------------------------\ny_pred_train_log = lin_model.predict(X_pca)\ny_pred_train = np.expm1(y_pred_train_log)  # invert log-transform\n\n# Clip negative values (just in case)\ny_pred_train = np.maximum(0, y_pred_train)\n\n# Evaluate\nrmse_train = np.sqrt(mean_squared_error(y, y_pred_train))\nr2_train = r2_score(y, y_pred_train)\n\nprint(f\"Training RMSE: {rmse_train}\")\nprint(f\"Training R²: {r2_train}\")\n\n# -----------------------------\n# Predict on test data\n# -----------------------------\n# Keep Id separately\ntest_ids = test_data_encoded['Id']\ntest_features = test_data_encoded.drop('Id', axis=1)\n\n# Ensure test features have same columns as training\ntest_features = test_features.reindex(columns=X.columns, fill_value=0)\n\n# Scale and apply PCA\nX_test_scaled = scaler.transform(test_features)\nX_test_pca = pca.transform(X_test_scaled)\n\n# Predict and invert log-transform\ny_pred_test_log = lin_model.predict(X_test_pca)\ny_pred_test = np.expm1(y_pred_test_log)\ny_pred_test = np.maximum(0, y_pred_test)  # clip negatives\n\n# Create final predictions DataFrame with Id and HotelValue\npredictions_df = pd.DataFrame({\n    'Id': test_ids,\n    'HotelValue': y_pred_test\n})\n\nprint(predictions_df.head())\n\n# Optional: save predictions\npredictions_df.to_csv('hotel_value_predictions.csv', index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Non-linear response and optimizing hyperparameters using Bayesian approach**","metadata":{}},{"cell_type":"code","source":"# View the first few predictions\nprint(predictions_df.head())\n\n# Save to CSV for later use\npredictions_df.to_csv('hotel_value_predictions.csv', index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print number of rows and columns\nprint(predictions_df.shape)\n\n# Optional: just number of rows\nprint(\"Number of rows:\", predictions_df.shape[0])\n\n# Optional: just number of columns\nprint(\"Number of columns:\", predictions_df.shape[1])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = ada_model.predict(test_data_encoded)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Predicting values for Adaboost**","metadata":{}},{"cell_type":"code","source":"# Assuming `test_data` (or `test_processed`) still has the 'Id' column\nsubmission_df = pd.DataFrame({\n    'Id': test_data_copy['Id'],       # bring back Hotel ID\n    'HotelValue': y_pred        # predicted values\n})\n\n# Save to CSV\nsubmission_df.to_csv('hotelvalue_submission.csv', index=False)\n\n# Preview first few rows\nprint(submission_df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}